{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Text Preprocessing\n",
    "\n",
    "Nikolaou Nikolaos - DSC18014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nick/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import requests\n",
    "from readability.readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import os\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "import nltk.data\n",
    "# nltk.data.path.append(\"/mnt/c/users/nick/Google Drive/Study/DataScience/2. NLP/Assignments/Exercise 1/.nltk_data/\")\n",
    "# nltk.data.path.append(\"C:/Users/nnikolao/PycharmProjects/.nltk_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the word count and vocabulary of this Web page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Artificial_neural_network'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "html = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(f):\n",
    "    paper = Document(f)\n",
    "    soup = BeautifulSoup(paper.summary())\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\",\"math\"]):\n",
    "        script.decompose()    # rip it out\n",
    "    output = [paper.title()]\n",
    "    for tag in soup.find_all(TAGS):\n",
    "        # Get the HTML node text\n",
    "        paragraph = tag.get_text()\n",
    "\n",
    "        # Sentence Tokenize\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            # Word Tokenize and Part of Speech Tagging\n",
    "            sentences[idx] = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "        # Yield a list of sentences (the paragraph); each sentence of\n",
    "        # which is a list of tuples in the form (token, tag).\n",
    "        yield sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"./nlp_data\")\n",
    "except OSError:  \n",
    "    print (\"Directory already exists\")\n",
    "outpath = os.path.join(\"./nlp_data/\", \"outfile\" + \".txt\")\n",
    "with codecs.open(outpath, 'w+', encoding='utf-8') as f:\n",
    "    # Write paragraphs double newline separated and sentences\n",
    "    # separated by a single newline. Also write token/tag pairs.\n",
    "    for paragraph in preprocess(html):\n",
    "        for sentence in paragraph:\n",
    "            f.write(\" \".join(\"%s/%s\" % (word, tag) for word, tag in sentence))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the Outfile, we have the preprocessed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(list(string.punctuation))          # Remove punctuation\n",
    "stopwords.extend([\"''\", '``', \"'s\", \"n't\", \"'ll\"])  # Custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the corpus\n",
    "corpus = nltk.corpus.TaggedCorpusReader(\"./nlp_data\",r'.*txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the interesting words from corpus\n",
    "words = [word.lower() for word in corpus.words() if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the words,sentences and tags\n",
    "tokens    = nltk.FreqDist(corpus.words())\n",
    "unigrams  = nltk.FreqDist(words)\n",
    "bigrams   = nltk.FreqDist(nltk.bigrams(words))\n",
    "tags      = nltk.FreqDist(tag for word, tag in corpus.tagged_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate stopwords\n",
    "for word in stopwords:\n",
    "    unigrams.pop(word, None)\n",
    "    bigrams.pop(word, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate the vocabulary and word count\n",
    "vocab     = len(tokens)            # The number of unique tokens\n",
    "sents     = len(corpus.sents())\n",
    "count     = sum(tokens.values())   # The word count for the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering the questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the word count and vocabulary of this Web page?\n",
      " This corpus contains 23153 words with a vocabulary of 4785 tokens.\n",
      "\n",
      "2. How many sentences are contained in the page?\n",
      " This corpus contains 1971 sentences.\n",
      "\n",
      "3. What is the lexical diversity of the page?\n",
      " The lexical diversity is 4.839\n"
     ]
    }
   ],
   "source": [
    "print(\"1. What is the word count and vocabulary of this Web page?\")\n",
    "print (\" This corpus contains %i words with a vocabulary of %i tokens.\"  % (count, vocab))\n",
    "\n",
    "print(\"\\n2. How many sentences are contained in the page?\")\n",
    "print (\" This corpus contains %i sentences.\"  % (sents))\n",
    "\n",
    "print(\"\\n3. What is the lexical diversity of the page?\")\n",
    "print (\" The lexical diversity is %0.3f\" % (float(count) / float(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. What are the 5 most common lexical categories (parts of speech)?\n",
      " The 5 most common tags are:\n",
      "    1. NNP (3517 samples)\n",
      "    2. NN (3284 samples)\n",
      "    3. JJ (1859 samples)\n",
      "    4. . (1792 samples)\n",
      "    5. IN (1672 samples)\n"
     ]
    }
   ],
   "source": [
    "print(\"4. What are the 5 most common lexical categories (parts of speech)?\")\n",
    "print (\" The 5 most common tags are:\")\n",
    "for idx, tag in enumerate(tags.most_common(5)):\n",
    "    print (\"    %i. %s (%i samples)\" % ((idx+1,) + tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. What are the 10 most common unigrams, the 10 most common bigrams?\n",
      " The 10 most common unigrams are:\n",
      "    1. neural (271 samples)\n",
      "    2. networks (213 samples)\n",
      "    3. learning (191 samples)\n",
      "    4. network (129 samples)\n",
      "    5. deep (90 samples)\n",
      "    6. artificial (80 samples)\n",
      "    7. function (72 samples)\n",
      "    8. edit (70 samples)\n",
      "    9. recognition (55 samples)\n",
      "    10. using (55 samples)\n",
      "\n",
      " The 10 most common bigrams are:\n",
      "    1. ('neural', 'networks') (147 samples)\n",
      "    2. ('neural', 'network') (68 samples)\n",
      "    3. ('artificial', 'neural') (58 samples)\n",
      "    4. ('machine', 'learning') (23 samples)\n",
      "    5. ('deep', 'learning') (23 samples)\n",
      "    6. ('international', 'conference') (17 samples)\n",
      "    7. ('cost', 'function') (16 samples)\n",
      "    8. ('pattern', 'recognition') (14 samples)\n",
      "    9. ('ieee', 'transactions') (13 samples)\n",
      "    10. ('recurrent', 'neural') (12 samples)\n"
     ]
    }
   ],
   "source": [
    "print(\"5. What are the 10 most common unigrams, the 10 most common bigrams?\")\n",
    "print (\" The 10 most common unigrams are:\")\n",
    "for idx, tag in enumerate(unigrams.most_common(10)):\n",
    "    print (\"    %i. %s (%i samples)\" % ((idx+1,) + tag))\n",
    "print (\"\\n The 10 most common bigrams are:\")\n",
    "for idx, tag in enumerate(bigrams.most_common(10)):\n",
    "    print (\"    %i. %s (%i samples)\" % ((idx+1,) + tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. How many nouns are in the page?\n",
      " There are 8044 nouns in the corpus\n"
     ]
    }
   ],
   "source": [
    "print(\"6. How many nouns are in the page?\")\n",
    "print (\" There are %i nouns in the corpus\" % sum(val for key,val in tags.items() if key.startswith('N')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_outpath = os.path.join(\"./nlp_data/\", \"csv_outfile\" + \".csv\")\n",
    "with codecs.open(csv_outpath, 'w', encoding='utf-8-sig') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL, dialect='excel')\n",
    "    csv_writer.writerow(['Νικόλαος', 'Νικολάου','“Μεταπτυχιακό στην Επιστήμη Δεδομένων”'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
