{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis \n",
    "\n",
    "## Project Preparation\n",
    "\n",
    "1. Load libraries\n",
    "2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/twitter-2013train-A.tsv', './data/twitter-2015train-A.tsv', './data/twitter-2016train-A.tsv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "fpattern = './data/twitter-20*train-*.tsv'\n",
    "filenames = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created a pattern for the filenames of our data we can start loading our data to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16173 entries, 0 to 16172\n",
      "Data columns (total 3 columns):\n",
      "id       16173 non-null int64\n",
      "tag      16173 non-null object\n",
      "tweet    16173 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 379.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id       tag  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                               tweet  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "1                                      Not Available  \n",
       "2                                      Not Available  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "4                                      Not Available  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['id', 'tag', 'tweet']\n",
    "df = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in filenames], ignore_index=True, sort=True)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all our train data are stored in a dataframe. However we can see that some tweets Not Available. This information is considered not usefull for our future classifier, so we must remove these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12016 entries, 0 to 16172\n",
      "Data columns (total 3 columns):\n",
      "id       12016 non-null int64\n",
      "tag      12016 non-null object\n",
      "tweet    12016 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 375.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>264094586689953794</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>254941790757601280</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id       tag  \\\n",
       "0  264183816548130816  positive   \n",
       "3  264249301910310912  negative   \n",
       "6  264105751826538497  positive   \n",
       "7  264094586689953794  negative   \n",
       "9  254941790757601280  negative   \n",
       "\n",
       "                                               tweet  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "6  with J Davlar 11th. Main rivals are team Polan...  \n",
       "7  Talking about ACT's &amp;&amp; SAT's, deciding...  \n",
       "9  They may have a SuperBowl in Dallas, but Dalla...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows having 'Not Available'...\n",
    "df = df[df.tweet != 'Not Available']\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to train a classifier that when given a new text string aka tweet, to be able to classify it by our given tag, which is positive, neutral or negative. \n",
    "Before diving into the exploratory analysis, we must define our features. We must split our tweets into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# turn a document into a list of clean tokens\n",
    "def clean_doc(doc):\n",
    "    # Remove links...\n",
    "    doc = re.sub(\"\\w+:\\/\\/\\S+\", \" \", doc)\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this functions to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12016 entries, 0 to 16172\n",
      "Data columns (total 6 columns):\n",
      "id               12016 non-null int64\n",
      "tag              12016 non-null object\n",
      "tweet            12016 non-null object\n",
      "tokens           12016 non-null object\n",
      "vector_tokens    12016 non-null object\n",
      "btag             12016 non-null int8\n",
      "dtypes: int64(1), int8(1), object(4)\n",
      "memory usage: 895.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tag\n",
       "negative    1671\n",
       "neutral     5181\n",
       "positive    5164\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['tokens'] = np.array([ clean_doc(tweet) for tweet in df.tweet ])\n",
    "df.info()  \n",
    "df.head()\n",
    "df.groupby('tag').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this procedure for dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpattern = './data/twitter-20*dev-*.tsv'\n",
    "devfs    = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "fpattern = './data/twitter-20*test-*.tsv'\n",
    "testfs   = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "df_dev   = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in devfs],  ignore_index=True, sort=True)\n",
    "df_test  = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in testfs], ignore_index=True, sort=True)\n",
    "df_dev   = df_dev[df_dev.tweet != 'Not Available']\n",
    "df_test  = df_test[df_test.tweet != 'Not Available']\n",
    "df_dev['tokens']  = np.array([ clean_doc(tweet) for tweet in df_dev.tweet ])\n",
    "df_test['tokens'] = np.array([ clean_doc(tweet) for tweet in df_test.tweet ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract our vocabulary, we need to iterate through all the tokens that are found and count the occurencies of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets:  30790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('may', 4040),\n",
       " ('tomorrow', 3942),\n",
       " ('The', 1934),\n",
       " ('Im', 1754),\n",
       " ('going', 1704),\n",
       " ('amp', 1687),\n",
       " ('see', 1667),\n",
       " ('day', 1667),\n",
       " ('Friday', 1648),\n",
       " ('like', 1582)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "vocabulary = Counter()\n",
    "for tweet_tokens in itertools.chain(df.tokens, df_dev.tokens, df_test.tokens):\n",
    "    vocabulary.update(tweet_tokens)\n",
    "\n",
    "print('Total tweets: ', sum(1 for _ in itertools.chain(df.tokens, df_dev.tokens, df_test.tokens)))\n",
    "vocabulary.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert our tweet's from a list of tokens to a vector of discrete tokens, using our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gas house hit Im going Chapel Hill Sat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_to_vector_words(tokens, vocabulary):\n",
    "    tokens = [w for w in tokens if w in vocabulary]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(df.tweet[0])\n",
    "token_to_vector_words(df.tokens[0], vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we apply this to all our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12016 entries, 0 to 16172\n",
      "Data columns (total 5 columns):\n",
      "id               12016 non-null int64\n",
      "tag              12016 non-null object\n",
      "tweet            12016 non-null object\n",
      "tokens           12016 non-null object\n",
      "vector_tokens    12016 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 883.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "      <td>[Gas, house, hit, Im, going, Chapel, Hill, Sat]</td>\n",
       "      <td>Gas house hit Im going Chapel Hill Sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "      <td>[Iranian, general, says, Israels, Iron, Dome, ...</td>\n",
       "      <td>Iranian general says Israels Iron Dome cant de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "      <td>[Davlar, Main, rivals, team, Poland, Hopefully...</td>\n",
       "      <td>Davlar Main rivals team Poland Hopefully make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>264094586689953794</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "      <td>[Talking, ACTs, ampamp, SATs, deciding, want, ...</td>\n",
       "      <td>Talking ACTs ampamp SATs deciding want go coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>254941790757601280</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "      <td>[They, may, SuperBowl, Dallas, Dallas, aint, w...</td>\n",
       "      <td>They may SuperBowl Dallas Dallas aint winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id       tag  \\\n",
       "0  264183816548130816  positive   \n",
       "3  264249301910310912  negative   \n",
       "6  264105751826538497  positive   \n",
       "7  264094586689953794  negative   \n",
       "9  254941790757601280  negative   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...   \n",
       "3  Iranian general says Israel's Iron Dome can't ...   \n",
       "6  with J Davlar 11th. Main rivals are team Polan...   \n",
       "7  Talking about ACT's &amp;&amp; SAT's, deciding...   \n",
       "9  They may have a SuperBowl in Dallas, but Dalla...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0    [Gas, house, hit, Im, going, Chapel, Hill, Sat]   \n",
       "3  [Iranian, general, says, Israels, Iron, Dome, ...   \n",
       "6  [Davlar, Main, rivals, team, Poland, Hopefully...   \n",
       "7  [Talking, ACTs, ampamp, SATs, deciding, want, ...   \n",
       "9  [They, may, SuperBowl, Dallas, Dallas, aint, w...   \n",
       "\n",
       "                                       vector_tokens  \n",
       "0             Gas house hit Im going Chapel Hill Sat  \n",
       "3  Iranian general says Israels Iron Dome cant de...  \n",
       "6  Davlar Main rivals team Poland Hopefully make ...  \n",
       "7  Talking ACTs ampamp SATs deciding want go coll...  \n",
       "9  They may SuperBowl Dallas Dallas aint winning ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vector_tokens']      = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df.tokens ])\n",
    "df_dev['vector_tokens']  = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df_dev.tokens ])\n",
    "df_test['vector_tokens'] = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df_test.tokens ])\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our Labels are categorical, we need to convert our classes to numeric tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector_tokens</th>\n",
       "      <th>btag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>638060586258038784</td>\n",
       "      <td>neutral</td>\n",
       "      <td>05 Beat it - Michael Jackson - Thriller (25th ...</td>\n",
       "      <td>[Beat, Michael, Jackson, Thriller, Anniversary...</td>\n",
       "      <td>Beat Michael Jackson Thriller Anniversary Edit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>638061181823922176</td>\n",
       "      <td>positive</td>\n",
       "      <td>Jay Z joins Instagram with nostalgic tribute t...</td>\n",
       "      <td>[Jay, joins, Instagram, nostalgic, tribute, Mi...</td>\n",
       "      <td>Jay joins Instagram nostalgic tribute Michael ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>638083821364244480</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Michael Jackson: Bad 25th Anniversary Edition ...</td>\n",
       "      <td>[Michael, Jackson, Bad, Anniversary, Edition, ...</td>\n",
       "      <td>Michael Jackson Bad Anniversary Edition Pictur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>638125563790557184</td>\n",
       "      <td>positive</td>\n",
       "      <td>18th anniv of Princess Diana's death. I still ...</td>\n",
       "      <td>[anniv, Princess, Dianas, death, still, want, ...</td>\n",
       "      <td>anniv Princess Dianas death still want believe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>638130776727535617</td>\n",
       "      <td>positive</td>\n",
       "      <td>@oridaganjazz The 1st time I heard Michael Jac...</td>\n",
       "      <td>[oridaganjazz, The, time, heard, Michael, Jack...</td>\n",
       "      <td>oridaganjazz The time heard Michael Jackson si...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>638162155250954241</td>\n",
       "      <td>negative</td>\n",
       "      <td>@etbowser do u enjoy his 2nd rate Michael Jack...</td>\n",
       "      <td>[etbowser, enjoy, rate, Michael, Jackson, bit,...</td>\n",
       "      <td>etbowser enjoy rate Michael Jackson bit Honest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id       tag  \\\n",
       "0  638060586258038784   neutral   \n",
       "1  638061181823922176  positive   \n",
       "2  638083821364244480   neutral   \n",
       "4  638125563790557184  positive   \n",
       "5  638130776727535617  positive   \n",
       "8  638162155250954241  negative   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  05 Beat it - Michael Jackson - Thriller (25th ...   \n",
       "1  Jay Z joins Instagram with nostalgic tribute t...   \n",
       "2  Michael Jackson: Bad 25th Anniversary Edition ...   \n",
       "4  18th anniv of Princess Diana's death. I still ...   \n",
       "5  @oridaganjazz The 1st time I heard Michael Jac...   \n",
       "8  @etbowser do u enjoy his 2nd rate Michael Jack...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Beat, Michael, Jackson, Thriller, Anniversary...   \n",
       "1  [Jay, joins, Instagram, nostalgic, tribute, Mi...   \n",
       "2  [Michael, Jackson, Bad, Anniversary, Edition, ...   \n",
       "4  [anniv, Princess, Dianas, death, still, want, ...   \n",
       "5  [oridaganjazz, The, time, heard, Michael, Jack...   \n",
       "8  [etbowser, enjoy, rate, Michael, Jackson, bit,...   \n",
       "\n",
       "                                       vector_tokens  btag  \n",
       "0  Beat Michael Jackson Thriller Anniversary Edit...     1  \n",
       "1  Jay joins Instagram nostalgic tribute Michael ...     2  \n",
       "2  Michael Jackson Bad Anniversary Edition Pictur...     1  \n",
       "4  anniv Princess Dianas death still want believe...     2  \n",
       "5  oridaganjazz The time heard Michael Jackson si...     2  \n",
       "8  etbowser enjoy rate Michael Jackson bit Honest...     0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map tag from class (positive, negative) to numbers...\n",
    "df['btag']      = df.tag.astype('category').cat.codes\n",
    "df_dev['btag']  = df_dev.tag.astype('category').cat.codes\n",
    "df_test['btag'] = df_test.tag.astype('category').cat.codes\n",
    "df_dev.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To vectorize our data we will use pre-trained embeddings**\n",
    "Word2Vec: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our vocabulary we will find the pre-trained embeddings and create an embedding matrix that will be used to fit our classification algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23740, 300)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.02001953  0.01135254  0.18847656 ... -0.02197266  0.07666016\n",
      "  -0.16894531]\n",
      " [ 0.05981445  0.02380371  0.06738281 ... -0.10986328 -0.00872803\n",
      "   0.03710938]\n",
      " ...\n",
      " [ 0.22594667 -0.22594741  0.18940675 ... -0.17294055  0.42542338\n",
      "  -0.7116653 ]\n",
      " [-0.04418945 -0.0177002  -0.06982422 ... -0.10791016  0.01586914\n",
      "   0.0378418 ]\n",
      " [-0.65509801  0.03796282  0.39025736 ...  0.34383929 -0.22580719\n",
      "   0.76601887]]\n",
      "[2625, 141, 358, 7, 6, 2340, 1249, 49]\n",
      "['Make', 'Sure', 'To', 'Come', 'To', 'The', 'Bob', 'Jones', 'Game', 'Friday', 'Free', 'Hot', 'Dogs', 'Hamburgers', 'amp', 'Food', 'outside', 'gate', 'amp', 'watch', 'Bob', 'Jones', 'take', 'Austin', 'High']\n",
      "[34, 119, 327, 29, 327, 5, 192, 1542, 15, 10, 96, 470, 1564, 14202, 11, 635, 1020, 3845, 11, 24, 192, 1542, 85, 1321, 465]\n",
      "Longest tweet (in words):  25\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.vector_tokens)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dimension = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector;#[:embedding_dimension]\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dimension)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_sequences(df.vector_tokens)\n",
    "Ytrain = df.btag\n",
    "Xtest  = tokenizer.texts_to_sequences(df_test.vector_tokens)\n",
    "Ytest  = df_test.btag\n",
    "from keras.utils import np_utils\n",
    "Ytrain_one_hot = np_utils.to_categorical(Ytrain)\n",
    "Ytest_one_hot  = np_utils.to_categorical(Ytest)\n",
    "print(Xtrain[0])\n",
    "\n",
    "## Get the longest tweet...\n",
    "longest = max(df.tokens,key=len)\n",
    "print(longest)\n",
    "longest = max(Xtrain,key=len)\n",
    "print(longest)\n",
    "longest = len(longest)\n",
    "print(\"Longest tweet (in words): \", longest)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all tweets lengths equal to the longest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2625, 141, 358, 7, 6, 2340, 1249, 49]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 2625  141  358    7    6 2340 1249   49]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(Xtrain[0])\n",
    "Xtrain = pad_sequences(Xtrain, maxlen=longest)\n",
    "Xtest  = pad_sequences(Xtest,  maxlen=longest)\n",
    "print(Xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23740\n"
     ]
    }
   ],
   "source": [
    "n_words = embedding_matrix.shape[0]\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(Ytrain_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(Ytrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question A\n",
    "# Supervised Classifier Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's try to train a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 25, 300)           7122000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 7500)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 22503     \n",
      "=================================================================\n",
      "Total params: 7,144,503\n",
      "Trainable params: 22,503\n",
      "Non-trainable params: 7,122,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7088b9a71b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Flatten, Embedding, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# define network\n",
    "model = Sequential()\n",
    "model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix], input_length=longest, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "# compile network\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "model.summary()\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to fit our data to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 1s - loss: 0.1170 - acc: 0.7813\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.1162 - acc: 0.7841\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.1154 - acc: 0.7856\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.1146 - acc: 0.7876\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.1138 - acc: 0.7895\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.1130 - acc: 0.7923\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.1123 - acc: 0.7934\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.1116 - acc: 0.7949\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.1110 - acc: 0.7967\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.1103 - acc: 0.7980\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.1097 - acc: 0.8001\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.1090 - acc: 0.8010\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.1084 - acc: 0.8025\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.1078 - acc: 0.8042\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.1073 - acc: 0.8049\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.1067 - acc: 0.8058\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.1062 - acc: 0.8088\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.1056 - acc: 0.8112\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.1051 - acc: 0.8123\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.1046 - acc: 0.8129\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.1041 - acc: 0.8151\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.1036 - acc: 0.8167\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.1031 - acc: 0.8171\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.1027 - acc: 0.8196\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.1022 - acc: 0.8199\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.1017 - acc: 0.8207\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.1013 - acc: 0.8204\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.1009 - acc: 0.8227\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.1005 - acc: 0.8243\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.1000 - acc: 0.8257\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.0996 - acc: 0.8270\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.0992 - acc: 0.8265\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.0988 - acc: 0.8278\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.0985 - acc: 0.8303\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.0981 - acc: 0.8308\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.0977 - acc: 0.8316\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.0973 - acc: 0.8313\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.0970 - acc: 0.8326\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.0966 - acc: 0.8332\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.0963 - acc: 0.8353\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.0959 - acc: 0.8350\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.0956 - acc: 0.8362\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.0953 - acc: 0.8374\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.0949 - acc: 0.8384\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.0946 - acc: 0.8385\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.0943 - acc: 0.8392\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.0940 - acc: 0.8411\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.0937 - acc: 0.8409\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.0934 - acc: 0.8422\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.0931 - acc: 0.8426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f638e561748>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "model.fit(K.cast_to_floatx(Xtrain), K.cast_to_floatx(Ytrain_one_hot), batch_size=10, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.383315\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(K.cast_to_floatx(Xtest), K.cast_to_floatx(Ytest_one_hot), verbose=2)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add onemore layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 25, 300)           7122000   \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 7500)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 22503     \n",
      "=================================================================\n",
      "Total params: 7,144,503\n",
      "Trainable params: 22,503\n",
      "Non-trainable params: 7,122,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7e5ca750b6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define network\n",
    "model = Sequential()\n",
    "model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix], input_length=longest, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "model.summary()\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " - 1s - loss: 0.5510 - acc: 0.7090\n",
      "Epoch 2/60\n",
      " - 1s - loss: 0.4410 - acc: 0.7919\n",
      "Epoch 3/60\n",
      " - 1s - loss: 0.3962 - acc: 0.8199\n",
      "Epoch 4/60\n",
      " - 1s - loss: 0.3664 - acc: 0.8387\n",
      "Epoch 5/60\n",
      " - 1s - loss: 0.3463 - acc: 0.8481\n",
      "Epoch 6/60\n",
      " - 1s - loss: 0.3285 - acc: 0.8590\n",
      "Epoch 7/60\n",
      " - 1s - loss: 0.3152 - acc: 0.8681\n",
      "Epoch 8/60\n",
      " - 1s - loss: 0.3028 - acc: 0.8734\n",
      "Epoch 9/60\n",
      " - 1s - loss: 0.2928 - acc: 0.8791\n",
      "Epoch 10/60\n",
      " - 1s - loss: 0.2853 - acc: 0.8839\n",
      "Epoch 11/60\n",
      " - 1s - loss: 0.2763 - acc: 0.8900\n",
      "Epoch 12/60\n",
      " - 1s - loss: 0.2696 - acc: 0.8918\n",
      "Epoch 13/60\n",
      " - 1s - loss: 0.2632 - acc: 0.8960\n",
      "Epoch 14/60\n",
      " - 1s - loss: 0.2573 - acc: 0.8984\n",
      "Epoch 15/60\n",
      " - 1s - loss: 0.2524 - acc: 0.9017\n",
      "Epoch 16/60\n",
      " - 1s - loss: 0.2474 - acc: 0.9025\n",
      "Epoch 17/60\n",
      " - 1s - loss: 0.2427 - acc: 0.9055\n",
      "Epoch 18/60\n",
      " - 1s - loss: 0.2388 - acc: 0.9068\n",
      "Epoch 19/60\n",
      " - 1s - loss: 0.2351 - acc: 0.9105\n",
      "Epoch 20/60\n",
      " - 1s - loss: 0.2310 - acc: 0.9112\n",
      "Epoch 21/60\n",
      " - 1s - loss: 0.2275 - acc: 0.9141\n",
      "Epoch 22/60\n",
      " - 1s - loss: 0.2237 - acc: 0.9171\n",
      "Epoch 23/60\n",
      " - 1s - loss: 0.2221 - acc: 0.9168\n",
      "Epoch 24/60\n",
      " - 1s - loss: 0.2180 - acc: 0.9177\n",
      "Epoch 25/60\n",
      " - 1s - loss: 0.2157 - acc: 0.9196\n",
      "Epoch 26/60\n",
      " - 1s - loss: 0.2134 - acc: 0.9208\n",
      "Epoch 27/60\n",
      " - 1s - loss: 0.2099 - acc: 0.9234\n",
      "Epoch 28/60\n",
      " - 1s - loss: 0.2080 - acc: 0.9232\n",
      "Epoch 29/60\n",
      " - 1s - loss: 0.2053 - acc: 0.9254\n",
      "Epoch 30/60\n",
      " - 1s - loss: 0.2035 - acc: 0.9257\n",
      "Epoch 31/60\n",
      " - 1s - loss: 0.2010 - acc: 0.9268\n",
      "Epoch 32/60\n",
      " - 1s - loss: 0.1992 - acc: 0.9271\n",
      "Epoch 33/60\n",
      " - 1s - loss: 0.1968 - acc: 0.9294\n",
      "Epoch 34/60\n",
      " - 1s - loss: 0.1952 - acc: 0.9294\n",
      "Epoch 35/60\n",
      " - 1s - loss: 0.1930 - acc: 0.9318\n",
      "Epoch 36/60\n",
      " - 1s - loss: 0.1905 - acc: 0.9326\n",
      "Epoch 37/60\n",
      " - 1s - loss: 0.1897 - acc: 0.9327\n",
      "Epoch 38/60\n",
      " - 1s - loss: 0.1880 - acc: 0.9341\n",
      "Epoch 39/60\n",
      " - 1s - loss: 0.1855 - acc: 0.9347\n",
      "Epoch 40/60\n",
      " - 1s - loss: 0.1846 - acc: 0.9340\n",
      "Epoch 41/60\n",
      " - 1s - loss: 0.1828 - acc: 0.9359\n",
      "Epoch 42/60\n",
      " - 1s - loss: 0.1814 - acc: 0.9364\n",
      "Epoch 43/60\n",
      " - 1s - loss: 0.1792 - acc: 0.9384\n",
      "Epoch 44/60\n",
      " - 1s - loss: 0.1783 - acc: 0.9384\n",
      "Epoch 45/60\n",
      " - 1s - loss: 0.1770 - acc: 0.9384\n",
      "Epoch 46/60\n",
      " - 1s - loss: 0.1760 - acc: 0.9386\n",
      "Epoch 47/60\n",
      " - 1s - loss: 0.1730 - acc: 0.9398\n",
      "Epoch 48/60\n",
      " - 1s - loss: 0.1736 - acc: 0.9398\n",
      "Epoch 49/60\n",
      " - 1s - loss: 0.1715 - acc: 0.9408\n",
      "Epoch 50/60\n",
      " - 1s - loss: 0.1704 - acc: 0.9414\n",
      "Epoch 51/60\n",
      " - 1s - loss: 0.1692 - acc: 0.9419\n",
      "Epoch 52/60\n",
      " - 1s - loss: 0.1678 - acc: 0.9436\n",
      "Epoch 53/60\n",
      " - 1s - loss: 0.1668 - acc: 0.9434\n",
      "Epoch 54/60\n",
      " - 1s - loss: 0.1651 - acc: 0.9429\n",
      "Epoch 55/60\n",
      " - 1s - loss: 0.1644 - acc: 0.9448\n",
      "Epoch 56/60\n",
      " - 1s - loss: 0.1638 - acc: 0.9436\n",
      "Epoch 57/60\n",
      " - 1s - loss: 0.1627 - acc: 0.9459\n",
      "Epoch 58/60\n",
      " - 1s - loss: 0.1607 - acc: 0.9457\n",
      "Epoch 59/60\n",
      " - 1s - loss: 0.1604 - acc: 0.9458\n",
      "Epoch 60/60\n",
      " - 1s - loss: 0.1590 - acc: 0.9487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64a78ca470>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "model.fit(K.cast_to_floatx(Xtrain), K.cast_to_floatx(Ytrain_one_hot), batch_size=10, epochs=60, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 66.436221\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(K.cast_to_floatx(Xtest), K.cast_to_floatx(Ytest_one_hot), verbose=2)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Let's try several classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a list, with one item per algorithm. Each item has a name, and a classifier object.\n",
    "\n",
    "models = []\n",
    "models.append(('LR',  LogisticRegression(solver='liblinear', multi_class='auto')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('kNN', KNeighborsClassifier()))\n",
    "models.append(('DT',  DecisionTreeClassifier()))\n",
    "models.append(('NB',  GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LR: 0.387996 (+/- 0.035956)\n",
      "LDA: 0.387613 (+/- 0.033707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN: 0.436849 (+/- 0.017242)\n",
      " DT: 0.448094 (+/- 0.030456)\n",
      " NB: 0.235622 (+/- 0.092053)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.267242 (+/- 0.082280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# We are going to evaluate all classifiers, and store results in two lists:\n",
    "results = []\n",
    "names   = []\n",
    "for name, model in models:\n",
    "  kfold = KFold(n_splits=10, random_state=7)\n",
    "  cv_results = cross_val_score(model, Xtrain, Ytrain, cv=kfold, scoring='f1_weighted')\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  print(\"%03s: %f (+/- %f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4406354990166544\n"
     ]
    }
   ],
   "source": [
    "# Let's create an ensemble with thealgorithms that performed best!\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models2 = []\n",
    "models2.append(('LR', models[0][1]))\n",
    "models2.append(('kNN', models[2][1]))\n",
    "models2.append(('DT', models[3][1]))\n",
    "ensemble = VotingClassifier(models2)\n",
    "results = cross_val_score(ensemble, Xtrain, Ytrain, cv=kfold, scoring='f1_weighted')\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4124208447103933\n"
     ]
    }
   ],
   "source": [
    "ensemble.fit(Xtrain,Ytrain)\n",
    "print(ensemble.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's optimize these three to achieve an even better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4388315579227696\n",
      "{'C': 0.1, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "params = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "gridlogreg = GridSearchCV(models[0][1], params)\n",
    "gridlogreg.fit(Xtrain, Ytrain)\n",
    "print(gridlogreg.best_score_)\n",
    "print(gridlogreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4509820239680426\n",
      "{'algorithm': 'auto', 'leaf_size': 3, 'n_jobs': -1, 'n_neighbors': 7}\n"
     ]
    }
   ],
   "source": [
    "params = {'n_neighbors':[4,5,6,7],\n",
    "              'leaf_size':[1,3,5],\n",
    "              'algorithm':['auto', 'kd_tree'],\n",
    "              'n_jobs':[-1]}\n",
    "gridknn = GridSearchCV(models[2][1], params)\n",
    "gridknn.fit(Xtrain, Ytrain)\n",
    "print(gridknn.best_score_)\n",
    "print(gridknn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47594873501997337\n",
      "{'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "params = {'max_depth': np.arange(3, 10)}\n",
    "\n",
    "griddt = GridSearchCV(models[3][1], params)\n",
    "griddt.fit(Xtrain, Ytrain)\n",
    "print(griddt.best_score_)\n",
    "print(griddt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run again our Voting classifier this time with the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4405529238653859\n"
     ]
    }
   ],
   "source": [
    "models3 = []\n",
    "models3.append(('LR',  gridlogreg.best_estimator_))\n",
    "models3.append(('kNN', gridknn.best_estimator_))\n",
    "models3.append(('DT',  griddt.best_estimator_))\n",
    "ensemble2 = VotingClassifier(models3)\n",
    "results = cross_val_score(ensemble2, Xtrain, Ytrain, cv=kfold, scoring='f1_weighted')\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4484982280834253\n"
     ]
    }
   ],
   "source": [
    "ensemble2.fit(Xtrain,Ytrain)\n",
    "print(ensemble2.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a considerable improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question I will use a personal amateur project that did not use pretrained embeddings.\n",
    "On the other hand the tweets were collected via the twitter API and tweet vectorization was done with sklearn's tfidf.\n",
    "As a classifier, only logistic regression was used.\n",
    "I use this implementation to answer this question purely out of curiosity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity I will copy and paste here the necessary code skipping the collection part. The script will be available though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/nick/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# will hold text\n",
    "data = []\n",
    "# will hold label\n",
    "target = []\n",
    "# Read the data from tweets1.csv\n",
    "csv_file = open(\"./data/tweets1.csv\", \"r\", encoding=\"utf8\")\n",
    "reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "data_column = 4\n",
    "for row in reader:\n",
    "    data.append(row[data_column])\n",
    "    target.append(int(row[0]))\n",
    "\n",
    "# instantiate the TFIDF vecotorizer    \n",
    "extr = TfidfVectorizer(min_df=0, max_features=None, strip_accents='unicode', lowercase=True,\n",
    "                                   analyzer='word', token_pattern=r'\\w{3,}', ngram_range=(1, 1),\n",
    "                                   use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words=\"english\")\n",
    "# Vectorize our data\n",
    "transformed_data = extr.fit_transform(data)\n",
    "\n",
    "# Fit a Logistic regression model\n",
    "model = LogisticRegression(C=1.)\n",
    "model.fit(transformed_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score is:  0.4684877651469472\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "new_transformed_data = extr.transform(df_test.vector_tokens)\n",
    "Y_predicted = model.predict(new_transformed_data)\n",
    "f1score = f1_score(Ytest, Y_predicted, average=\"weighted\")\n",
    "print(\"F1_score is: \", f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a slightly improved result with this approach.\n",
    "Probably the fact that these vectors were trained on actual twitter data, in contrast to word2vec, made a difference.\n",
    "Of course the result is nowhere near to the result of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
